{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bengaliai', 'bengaliai-cv19', 'bengaliai-cv19.zip', 'bengaliai.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(r'C:\\Users\\Kaggle\\BengaliAI\\inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add heng's code to our envionment and import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hengs_path = r'C:\\Users\\Kaggle\\BengaliAI\\seResNext_gridmask'\n",
    "sys.path.append(hengs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib.get_backend :  module://ipykernel.pylab.backend_inline\n"
     ]
    }
   ],
   "source": [
    "from common  import *\n",
    "from model   import *\n",
    "from kaggle import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heng uses his own version of datasplit(which I have uploaded [here](https://www.kaggle.com/bibek777/hengdata)) in his codes. I tried using it but get memory error, maybe it's too large to load. So I have edited the dataloader in his code and use different split for train and valid dataset. The codes/ideas for dataloader is taken from this [kernel](https://www.kaggle.com/backaggle/catalyst-baseline). Also the dataset used in this kernel is taken from [here](https://www.kaggle.com/pestipeti/bengaliai), uploaded by Peter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"C:\\Users\\Kaggle\\BengaliAI\\inputs\\bengaliai-cv19/train.csv\")\n",
    "train_df['id'] = train_df['image_id'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "X, y = train_df[['id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic']]\\\n",
    ".values[:,0], train_df.values[:,1:]\n",
    "\n",
    "train_df['fold'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "nfold = 7   #### change folds\n",
    "seed=42\n",
    "mskf = MultilabelStratifiedKFold(n_splits=nfold, random_state=seed,shuffle=True)\n",
    "for i, (_, test_index) in enumerate(mskf.split(X, y)):\n",
    "    train_df.iloc[test_index, -1] = i\n",
    "    \n",
    "train_df['fold'] = train_df['fold'].astype('int')\n",
    "\n",
    "valid_df = train_df[train_df['fold']==4]   #### change folds\n",
    "train_df = train_df[train_df['fold']!=4]   #### change folds\n",
    "\n",
    "dropVar = ['id','fold']\n",
    "train_df.drop(dropVar,1,inplace=True)\n",
    "valid_df.drop(dropVar,1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Train_9</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>তেঁ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Train_21</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>ণ্টু</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Train_25</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ণ্ঠা</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Train_40</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ঙ্কি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Train_41</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>গু</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
       "9    Train_9             64                7                    1      তেঁ\n",
       "21  Train_21             60                4                    0     ণ্টু\n",
       "25  Train_25             61                1                    0     ণ্ঠা\n",
       "40  Train_40             32                2                    0     ঙ্কি\n",
       "41  Train_41             23                4                    0       গু"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = r\"C:\\Users\\Kaggle\\BengaliAI\\inputs\\bengaliai\\256_train\\256\"\n",
    "TASK_NAME = [ 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic' ]\n",
    "NUM_TASK = len(TASK_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "from albumentations.core.transforms_interface import DualTransform\n",
    "from albumentations.augmentations import functional as aa\n",
    "\n",
    "\n",
    "class GridMask(DualTransform):\n",
    "\n",
    "    def __init__(self, num_grid=3, fill_value=0, rotate=0, mode=0, always_apply=False, p=0.5):\n",
    "        super(GridMask, self).__init__(always_apply, p)\n",
    "        if isinstance(num_grid, int):\n",
    "            num_grid = (num_grid, num_grid)\n",
    "        if isinstance(rotate, int):\n",
    "            rotate = (-rotate, rotate)\n",
    "        self.num_grid = num_grid\n",
    "        self.fill_value = fill_value\n",
    "        self.rotate = rotate\n",
    "        self.mode = mode\n",
    "        self.masks = None\n",
    "        self.rand_h_max = []\n",
    "        self.rand_w_max = []\n",
    "\n",
    "    def init_masks(self, height, width):\n",
    "        if self.masks is None:\n",
    "            self.masks = []\n",
    "            n_masks = self.num_grid[1] - self.num_grid[0] + 1\n",
    "            for n, n_g in enumerate(range(self.num_grid[0], self.num_grid[1] + 1, 1)):\n",
    "                grid_h = height / n_g\n",
    "                grid_w = width / n_g\n",
    "                this_mask = np.ones((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n",
    "                for i in range(n_g + 1):\n",
    "                    for j in range(n_g + 1):\n",
    "                        this_mask[\n",
    "                             int(i * grid_h) : int(i * grid_h + grid_h / 2),\n",
    "                             int(j * grid_w) : int(j * grid_w + grid_w / 2)\n",
    "                        ] = self.fill_value\n",
    "                        if self.mode == 2:\n",
    "                            this_mask[\n",
    "                                 int(i * grid_h + grid_h / 2) : int(i * grid_h + grid_h),\n",
    "                                 int(j * grid_w + grid_w / 2) : int(j * grid_w + grid_w)\n",
    "                            ] = self.fill_value\n",
    "                \n",
    "                if self.mode == 1:\n",
    "                    this_mask = 1 - this_mask\n",
    "\n",
    "                self.masks.append(this_mask)\n",
    "                self.rand_h_max.append(grid_h)\n",
    "                self.rand_w_max.append(grid_w)\n",
    "\n",
    "    def apply(self, image, mask, rand_h, rand_w, angle, **params):\n",
    "        h, w = image.shape[:2]\n",
    "        mask = aa.rotate(mask, angle) if self.rotate[1] > 0 else mask\n",
    "        mask = mask[:,:,np.newaxis] if image.ndim == 3 else mask\n",
    "        image *= mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype)\n",
    "        return image\n",
    "\n",
    "#     def get_params_dependent_on_targets(self, params,img):\n",
    "#         img = params['image']\n",
    "    def get_params_dependent_on_targets(self,img):\n",
    "        height, width = img.shape[:2]\n",
    "        self.init_masks(height, width)\n",
    "\n",
    "        mid = np.random.randint(len(self.masks))\n",
    "        mask = self.masks[mid]\n",
    "        rand_h = np.random.randint(self.rand_h_max[mid])\n",
    "        rand_w = np.random.randint(self.rand_w_max[mid])\n",
    "        angle = np.random.randint(self.rotate[0], self.rotate[1]) if self.rotate[1] > 0 else 0\n",
    "\n",
    "        return {'mask': mask, 'rand_h': rand_h, 'rand_w': rand_w, 'angle': angle}\n",
    "\n",
    "    @property\n",
    "    def targets_as_params(self):\n",
    "        return ['image']\n",
    "\n",
    "    def get_transform_init_args_names(self):\n",
    "        return ('num_grid', 'fill_value', 'rotate', 'mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, data_path, augment=None):\n",
    "        self.image_ids = df['image_id'].values\n",
    "        self.grapheme_roots = df['grapheme_root'].values\n",
    "        self.vowel_diacritics = df['vowel_diacritic'].values\n",
    "        self.consonant_diacritics = df['consonant_diacritic'].values\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.augment = augment\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen = %d\\n'%len(self)\n",
    "        string += '\\n'\n",
    "        return string\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        image_id = self.image_ids[index]\n",
    "        grapheme_root = self.grapheme_roots[index]\n",
    "        vowel_diacritic = self.vowel_diacritics[index]\n",
    "        consonant_diacritic = self.consonant_diacritics[index]\n",
    "\n",
    "        image_id = os.path.join(self.data_path, image_id + '.png')\n",
    "\n",
    "        image = cv2.imread(image_id, 0)\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_GRAY2BGR)\n",
    "        image = image.astype(np.float32)/255\n",
    "        label = [grapheme_root, vowel_diacritic, consonant_diacritic]\n",
    "\n",
    "        infor = Struct(\n",
    "            index    = index,\n",
    "            image_id = image_id,\n",
    "        )\n",
    "\n",
    "        if self.augment is None:\n",
    "            return image, label, infor\n",
    "        else:\n",
    "            return self.augment(image, label, infor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    input = []\n",
    "    label = []\n",
    "    infor = []\n",
    "    for b in range(batch_size):\n",
    "        input.append(batch[b][0])\n",
    "        label.append(batch[b][1])\n",
    "        infor.append(batch[b][-1])\n",
    "\n",
    "    input = np.stack(input)\n",
    "    #input = input[...,::-1].copy()\n",
    "    input = input.transpose(0,3,1,2)\n",
    "\n",
    "    label = np.stack(label)\n",
    "\n",
    "    #----\n",
    "    input = torch.from_numpy(input).float()\n",
    "    truth = torch.from_numpy(label).long()\n",
    "    truth0, truth1, truth2 = truth[:,0],truth[:,1],truth[:,2]\n",
    "    truth = [truth0, truth1, truth2]\n",
    "    return input, truth, infor\n",
    "\n",
    "\n",
    "##############################################################\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    image = tensor.data.cpu().numpy()\n",
    "    image = image.transpose(0,2,3,1)\n",
    "    #image = image[...,::-1]\n",
    "    return image\n",
    "\n",
    "\n",
    "##############################################################\n",
    "\n",
    "def do_random_crop_rotate_rescale(\n",
    "    image,\n",
    "    mode={'rotate': 10,'scale': 0.1,'shift': 0.1}\n",
    "):\n",
    "\n",
    "    dangle = 0\n",
    "    dscale_x, dscale_y = 0,0\n",
    "    dshift_x, dshift_y = 0,0\n",
    "\n",
    "    for k,v in mode.items():\n",
    "        if   'rotate'== k:\n",
    "            dangle = np.random.uniform(-v, v)\n",
    "        elif 'scale' == k:\n",
    "            dscale_x, dscale_y = np.random.uniform(-1, 1, 2)*v\n",
    "        elif 'shift' == k:\n",
    "            dshift_x, dshift_y = np.random.uniform(-1, 1, 2)*v\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    #----\n",
    "\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    cos = np.cos(dangle/180*PI)\n",
    "    sin = np.sin(dangle/180*PI)\n",
    "    sx,sy = 1 + dscale_x, 1+ dscale_y #1,1 #\n",
    "    tx,ty = dshift_x*width, dshift_y*height\n",
    "\n",
    "    src = np.array([[-width/2,-height/2],[ width/2,-height/2],[ width/2, height/2],[-width/2, height/2]], np.float32)\n",
    "    src = src*[sx,sy]\n",
    "    x = (src*[cos,-sin]).sum(1)+width/2 +tx\n",
    "    y = (src*[sin, cos]).sum(1)+height/2+ty\n",
    "    src = np.column_stack([x,y])\n",
    "\n",
    "    dst = np.array([[0,0],[width,0],[width,height],[0,height]])\n",
    "    s = src.astype(np.float32)\n",
    "    d = dst.astype(np.float32)\n",
    "    transform = cv2.getPerspectiveTransform(s,d)\n",
    "    image = cv2.warpPerspective( image, transform, (width, height), flags=cv2.INTER_LINEAR,\n",
    "                                 borderMode=cv2.BORDER_CONSTANT, borderValue=(1,1,1))\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def do_random_log_contast(image, gain=[0.70, 1.30] ):\n",
    "    gain = np.random.uniform(gain[0],gain[1],1)\n",
    "    inverse = np.random.choice(2,1)\n",
    "\n",
    "    if inverse==0:\n",
    "        image = gain*np.log(image+1)\n",
    "    else:\n",
    "        image = gain*(2**image-1)\n",
    "\n",
    "    image = np.clip(image,0,1)\n",
    "    return image\n",
    "\n",
    "\n",
    "#https://github.com/albumentations-team/albumentations/blob/8b58a3dbd2f35558b3790a1dbff6b42b98e89ea5/albumentations/augmentations/transforms.py\n",
    "def do_grid_distortion(image, distort=0.25, num_step = 10):\n",
    "\n",
    "    # http://pythology.blogspot.sg/2014/03/interpolation-on-regular-distorted-grid.html\n",
    "    distort_x = [1 + random.uniform(-distort,distort) for i in range(num_step + 1)]\n",
    "    distort_y = [1 + random.uniform(-distort,distort) for i in range(num_step + 1)]\n",
    "\n",
    "    #---\n",
    "    height, width = image.shape[:2]\n",
    "    xx = np.zeros(width, np.float32)\n",
    "    step_x = width // num_step\n",
    "\n",
    "    prev = 0\n",
    "    for i, x in enumerate(range(0, width, step_x)):\n",
    "        start = x\n",
    "        end   = x + step_x\n",
    "        if end > width:\n",
    "            end = width\n",
    "            cur = width\n",
    "        else:\n",
    "            cur = prev + step_x * distort_x[i]\n",
    "        xx[start:end] = np.linspace(prev, cur, end - start)\n",
    "        prev = cur\n",
    "\n",
    "\n",
    "    yy = np.zeros(height, np.float32)\n",
    "    step_y = height // num_step\n",
    "\n",
    "    prev = 0\n",
    "    for idx, y in enumerate(range(0, height, step_y)):\n",
    "        start = y\n",
    "        end = y + step_y\n",
    "        if end > height:\n",
    "            end = height\n",
    "            cur = height\n",
    "        else:\n",
    "            cur = prev + step_y * distort_y[idx]\n",
    "\n",
    "        yy[start:end] = np.linspace(prev, cur, end - start)\n",
    "        prev = cur\n",
    "\n",
    "    map_x, map_y = np.meshgrid(xx, yy)\n",
    "    map_x = map_x.astype(np.float32)\n",
    "    map_y = map_y.astype(np.float32)\n",
    "    image = cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(1,1,1))\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "# ##---\n",
    "# #https://github.com/chainer/chainercv/blob/master/chainercv/links/model/ssd/transforms.py\n",
    "def do_random_contast(image, alpha=[0,1]):\n",
    "    beta  = 0\n",
    "    alpha = random.uniform(*alpha) + 1\n",
    "    image = image.astype(np.float32) * alpha + beta\n",
    "    image = np.clip(image,0,1)\n",
    "    return image\n",
    "\n",
    "def do_mixup(input, onehot,truth):\n",
    "    batch_size = len(input)\n",
    "\n",
    "    alpha = 0.4  #0.2,0.4\n",
    "    gamma = np.random.beta(alpha, alpha)\n",
    "    gamma = max(1-gamma,gamma)\n",
    "\n",
    "    # #mixup https://github.com/moskomule/mixup.pytorch/blob/master/main.py\n",
    "    perm = torch.randperm(batch_size).to(input.device)\n",
    "    perm_input  = input[perm]\n",
    "    perm_onehot = [t[perm] for t in onehot]\n",
    "    mix_input  = gamma*input + (1-gamma)*perm_input\n",
    "    mix_onehot = [gamma*t    + (1-gamma)*perm_t for t,perm_t in zip(truth,perm_onehot)]\n",
    "    return mix_input, mix_onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doGridmask(image,num_grid,rotate):\n",
    "    Gridmask=GridMask(num_grid=num_grid, fill_value=0, rotate=rotate, mode=0, always_apply=False, p=1)\n",
    "    out=Gridmask.get_params_dependent_on_targets(image)\n",
    "    mask = out['mask']\n",
    "    rand_h = out['rand_h']\n",
    "    rand_w = out['rand_w']\n",
    "    angle = out['angle']\n",
    "    image=Gridmask.apply(image,mask,rand_h,rand_w,angle)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "\n",
    "def train_augment(image, label, infor):\n",
    "    if np.random.rand()< 0.3:\n",
    "        image = do_random_crop_rotate_rescale(image, mode={'rotate': 17.5,'scale': 0.25,'shift': 0.08})\n",
    "    else:\n",
    "        image = doGridmask(image,5,25)\n",
    "    return image, label, infor\n",
    "\n",
    "def valid_augment(image, label, infor):\n",
    "    return image, label, infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.augmentations.transforms import *\n",
    "# train_augment = albumentations.Compose([GridMask(num_grid=3, p=1),Rotate(5,p=0.1),OpticalDistortion(p=0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------\n",
    "def do_valid(net, valid_loader, out_dir=None):\n",
    "    def criterion(logit, truth):\n",
    "        loss = []\n",
    "        for l,t in zip(logit,truth):\n",
    "            e = F.cross_entropy(l, t)\n",
    "            #e = cross_entropy_onehot_loss(l, t)   --- older version\n",
    "            loss.append(e)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    valid_loss = np.zeros(6, np.float32)\n",
    "    valid_num  = np.zeros_like(valid_loss)\n",
    "\n",
    "    valid_probability = [[],[],[],]\n",
    "    valid_truth = [[],[],[],]\n",
    "\n",
    "    for t, (input, truth, infor) in enumerate(valid_loader):\n",
    "\n",
    "        batch_size = len(infor)\n",
    "\n",
    "        net.eval()\n",
    "        input = input.cuda()\n",
    "        truth = [t.cuda() for t in truth]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit = data_parallel(net, input) #net(input)\n",
    "            probability = logit_to_probability(logit)\n",
    "\n",
    "            loss = criterion(logit, truth)\n",
    "            correct = metric(probability, truth)\n",
    "\n",
    "        #---\n",
    "        loss = [l.item() for l in loss]\n",
    "        l = np.array([ *loss, *correct, ])*batch_size\n",
    "        n = np.array([ 1, 1, 1, 1, 1, 1  ])*batch_size\n",
    "        valid_loss += l\n",
    "        valid_num  += n\n",
    "\n",
    "        #---\n",
    "        for i in range(NUM_TASK):\n",
    "            valid_probability[i].append(probability[i].data.cpu().numpy())\n",
    "            valid_truth[i].append(truth[i].data.cpu().numpy())\n",
    "\n",
    "        #print(valid_loss)\n",
    "        print('\\r %8d /%d'%(valid_num[0], len(valid_loader.dataset)),end='',flush=True)\n",
    "\n",
    "        pass  #-- end of one data loader --\n",
    "    assert(valid_num[0] == len(valid_loader.dataset))\n",
    "    valid_loss = valid_loss/(valid_num+1e-8)\n",
    "\n",
    "    #------\n",
    "    for i in range(NUM_TASK):\n",
    "        valid_probability[i] = np.concatenate(valid_probability[i])\n",
    "        valid_truth[i] = np.concatenate(valid_truth[i])\n",
    "    recall, avgerage_recall = compute_kaggle_metric(valid_probability, valid_truth)\n",
    "\n",
    "\n",
    "    return valid_loss, (recall, avgerage_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "def cutmix(data, targets1, targets2, targets3, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets1 = targets1[indices]\n",
    "    shuffled_targets2 = targets2[indices]\n",
    "    shuffled_targets3 = targets3[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "\n",
    "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, shuffled_targets3, lam]\n",
    "    return data, targets\n",
    "\n",
    "def mixup(data, targets1, targets2, targets3, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets1 = targets1[indices]\n",
    "    shuffled_targets2 = targets2[indices]\n",
    "    shuffled_targets3 = targets3[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, shuffled_targets3, lam]\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def cutmix_criterion(preds1,preds2,preds3, targets):\n",
    "    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2) + lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4) + lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)\n",
    "\n",
    "def mixup_criterion(preds1,preds2,preds3, targets):\n",
    "    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2) + lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4) + lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion_(logit, truth):\n",
    "        loss = []\n",
    "        for l,t in zip(logit,truth):\n",
    "            e = F.cross_entropy(l, t)\n",
    "            loss.append(e)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(testing=False,augment=None):\n",
    "\n",
    "    if testing:\n",
    "        out_dir = r'C:\\Users\\Kaggle\\BengaliAI\\testing'\n",
    "    else:\n",
    "        out_dir = r'C:\\Users\\Kaggle\\BengaliAI\\seResNext_gridmask_output_contin1'\n",
    "    initial_checkpoint = r'C:\\Users\\Kaggle\\BengaliAI\\seResNext_gridmask_output\\checkpoint\\00199999_model.pth'\n",
    "\n",
    "#     schduler = NullScheduler(lr=0.005)\n",
    "    schduler = CyclicScheduler0(min_lr=0.001, max_lr=0.01, period=10, ratio=1.0)\n",
    "   \n",
    "    iter_accum = 1\n",
    "    batch_size = 16 #8\n",
    "\n",
    "    ## setup  -----------------------------------------------------------------------------\n",
    "    for f in ['checkpoint','train','valid'] : os.makedirs(out_dir +'/'+f, exist_ok=True)\n",
    "        \n",
    "    log = Logger()\n",
    "    log.open(out_dir+'/log.train.txt',mode='a')\n",
    "    log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n",
    "    log.write('\\t%s\\n' % COMMON_STRING)\n",
    "    log.write('\\n')\n",
    "\n",
    "    log.write('\\tSEED         = %u\\n' % SEED)\n",
    "    log.write('\\tout_dir      = %s\\n' % out_dir)\n",
    "    log.write('\\n')\n",
    "\n",
    "\n",
    "    ## dataset ----------------------------------------\n",
    "    log.write('** dataset setting **\\n')\n",
    "\n",
    "    train_dataset = KaggleDataset(\n",
    "        df = train_df, \n",
    "        data_path = data_root,\n",
    "        augment = train_augment,\n",
    "    )\n",
    "    train_loader  = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler     = RandomSampler(train_dataset),\n",
    "        batch_size  = batch_size,\n",
    "        drop_last   = True,\n",
    "        num_workers = 0,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "\n",
    "\n",
    "    valid_dataset = KaggleDataset(\n",
    "        df = valid_df, \n",
    "        data_path = data_root,\n",
    "        augment = valid_augment,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        sampler     = SequentialSampler(valid_dataset),\n",
    "        batch_size  = 64,\n",
    "        drop_last   = False,\n",
    "        num_workers = 0,\n",
    "        pin_memory  = True,\n",
    "        collate_fn  = null_collate\n",
    "    )\n",
    "\n",
    "    assert(len(train_dataset)>=batch_size)\n",
    "    log.write('batch_size = %d\\n'%(batch_size))\n",
    "    log.write('train_dataset : \\n%s\\n'%(train_dataset))\n",
    "    log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n",
    "    log.write('\\n')\n",
    "\n",
    "    ## net ----------------------------------------\n",
    "    log.write('** net setting **\\n')\n",
    "    net = Net().cuda()\n",
    "    \n",
    "    \n",
    "    log.write('\\tinitial_checkpoint = %s\\n' % initial_checkpoint)\n",
    "\n",
    "    if initial_checkpoint is not None:\n",
    "        state_dict = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n",
    "        net.load_state_dict(state_dict,strict=True)  #True\n",
    "    else:\n",
    "        net.load_pretrain(is_print=False)\n",
    "        print(\"Loaded pretrained model\")\n",
    "\n",
    "\n",
    "    log.write('net=%s\\n'%(type(net)))\n",
    "    log.write('\\n')\n",
    "    \n",
    "    \n",
    "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=schduler(0), momentum=0., weight_decay=0.0)\n",
    "\n",
    "    num_iters   = 200*1000  # comment this for training longer\n",
    "    iter_smooth = 50\n",
    "    iter_log    = 250\n",
    "    iter_valid  = 500\n",
    "    iter_save   = [0, num_iters-1]\\\n",
    "                   + list(range(0, num_iters, 1000))#1*1000\n",
    "\n",
    "    start_iter = 0\n",
    "    start_epoch= 0\n",
    "    rate       = 0\n",
    "    if initial_checkpoint is not None:\n",
    "        initial_optimizer = initial_checkpoint.replace('_model.pth','_optimizer.pth')\n",
    "        if os.path.exists(initial_optimizer):\n",
    "            checkpoint  = torch.load(initial_optimizer)\n",
    "            start_iter  = checkpoint['iter' ]+1    ### critial to load and restart training from previous pt \n",
    "            start_epoch = checkpoint['epoch']+1    ### critial to load and restart training from previous pt\n",
    "        pass\n",
    "\n",
    "    log.write('optimizer\\n  %s\\n'%(optimizer))\n",
    "    log.write('schduler\\n  %s\\n'%(schduler))\n",
    "    log.write('\\n')\n",
    "\n",
    "    ## start training here! ##############################################\n",
    "    log.write('** start training here! **\\n')\n",
    "    log.write('   batch_size=%d,  iter_accum=%d\\n'%(batch_size,iter_accum))\n",
    "    log.write('                    |----------------------- VALID------------------------------------|------- TRAIN/BATCH -----------\\n')\n",
    "    log.write('rate    iter  epoch | kaggle                    | loss               acc              | loss             | time       \\n')\n",
    "    log.write('----------------------------------------------------------------------------------------------------------------------\\n')\n",
    "              #0.01000  26.2  15.1 | 0.971 : 0.952 0.992 0.987 | 0.22, 0.07, 0.07 : 0.94, 0.98, 0.98 | 0.37, 0.13, 0.13 | 0 hr 13 min\n",
    "\n",
    "    def message(rate, iter, epoch, kaggle, valid_loss, train_loss, batch_loss, mode='print'):\n",
    "        if mode==('print'):\n",
    "            asterisk = ' '\n",
    "            lossl = batch_loss\n",
    "            \n",
    "        if mode==('log'):\n",
    "            asterisk = '*' if iter in iter_save else ' '\n",
    "            lossl = train_loss\n",
    "        \n",
    "        text = \\\n",
    "            '%0.5f %5.1f%s %4.1f | '%(rate, iter/1000, asterisk, epoch,) +\\\n",
    "            '%0.3f : %0.3f %0.3f %0.3f | '%(kaggle[1],*kaggle[0]) +\\\n",
    "            '%4.2f, %4.2f, %4.2f : %4.2f, %4.2f, %4.2f | '%(*valid_loss,) +\\\n",
    "            '%4.2f, %4.2f, %4.2f |'%(*lossl,) +\\\n",
    "            '%s' % (time_to_str((timer() - start_timer),'min'))\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "    #----\n",
    "    kaggle = (0,0,0,0)\n",
    "    valid_loss = np.zeros(6,np.float32)\n",
    "    train_loss = np.zeros(3,np.float32)\n",
    "    batch_loss = np.zeros_like(train_loss)\n",
    "    iter = 0\n",
    "    i    = 0\n",
    "\n",
    "\n",
    "\n",
    "    start_timer = timer()\n",
    "    while  iter<num_iters:\n",
    "            \n",
    "        sum_train_loss = np.zeros_like(train_loss)\n",
    "        sum_train = np.zeros_like(train_loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for t, (input, truth, infor) in enumerate(train_loader):\n",
    "            batch_size = len(infor)\n",
    "            iter  = i + start_iter\n",
    "            epoch = (iter-start_iter)*batch_size/len(train_dataset) + start_epoch\n",
    "            \n",
    "            #if 0:\n",
    "            if not testing:\n",
    "                if (iter % iter_valid==0):\n",
    "                    valid_loss, kaggle = do_valid(net, valid_loader, out_dir) #\n",
    "                    pass\n",
    "\n",
    "                if (iter % iter_log==0):\n",
    "                    print('\\r',end='',flush=True)\n",
    "                    log.write(message(rate, iter, epoch, kaggle, valid_loss, train_loss, batch_loss,mode='log'))\n",
    "                    log.write('\\n')\n",
    "\n",
    "                if iter in iter_save:\n",
    "                    torch.save({\n",
    "                        #'optimizer': optimizer.state_dict(),\n",
    "                        'iter'     : iter,\n",
    "                        'epoch'    : epoch,\n",
    "                    }, out_dir +'/checkpoint/%08d_optimizer.pth'%(iter))\n",
    "                    if iter!=start_iter:\n",
    "                        torch.save(net.state_dict(),out_dir +'/checkpoint/%08d_model.pth'%(iter))\n",
    "                        pass\n",
    "\n",
    "            # learning rate schduler -------------\n",
    "            lr = schduler(iter)\n",
    "            if lr<0 : break\n",
    "            adjust_learning_rate(optimizer, lr)\n",
    "            rate = get_learning_rate(optimizer)\n",
    "\n",
    "            # one iteration update  -------------\n",
    "            #net.set_mode('train',is_freeze_bn=True)\n",
    "\n",
    "            net.train()\n",
    "            input = input.cuda()\n",
    "            truth = [t.cuda() for t in truth]\n",
    "            NUM_CLASS = (168,11,7)\n",
    "            \n",
    "    \n",
    "            logit = data_parallel(net, input)\n",
    "            probability = logit_to_probability(logit)\n",
    "\n",
    "\n",
    "            loss = criterion(logit, truth)\n",
    "            (( 4*loss[0]+loss[1]+loss[2] )/iter_accum).backward()\n",
    "            \n",
    "            if (iter % iter_accum)==0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            # print statistics  --------\n",
    "                \n",
    "            loss_ = [l.item() for l in loss]\n",
    "            l = np.array([ *loss_, ])*batch_size\n",
    "            n = np.array([ 1, 1, 1 ])*batch_size\n",
    "            batch_loss      = l/(n+1e-8)\n",
    "            sum_train_loss += l\n",
    "            sum_train      += n\n",
    "            if iter%iter_smooth == 0:\n",
    "                train_loss = sum_train_loss/(sum_train+1e-12)\n",
    "                sum_train_loss[...] = 0\n",
    "                sum_train[...]      = 0\n",
    "\n",
    "            print('\\r',end='',flush=True)\n",
    "            \n",
    "            print(message(rate, iter, epoch, kaggle, valid_loss, train_loss, batch_loss,mode='print'), end='',flush=True)\n",
    "            i=i+1\n",
    "\n",
    "        pass  #-- end of one data loader --\n",
    "    pass #-- end of all iterations --\n",
    "    \n",
    "    log.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [START 2020-02-12_20-17-27] ----------------------------------------------------------------\n",
      "\n",
      "\t@common.py:  \n",
      "\tset random seed\n",
      "\t\tSEED = 1581567447\n",
      "\tset cuda environment\n",
      "\t\ttorch.__version__              = 1.1.0\n",
      "\t\ttorch.version.cuda             = 9.0\n",
      "\t\ttorch.backends.cudnn.version() = 7005\n",
      "\t\tos['CUDA_VISIBLE_DEVICES']     = None\n",
      "\t\ttorch.cuda.device_count()      = 1\n",
      "\n",
      "\n",
      "\n",
      "\tSEED         = 1581567447\n",
      "\tout_dir      = C:\\Users\\Kaggle\\BengaliAI\\seResNext_gridmask_output_contin1\n",
      "\n",
      "** dataset setting **\n",
      "batch_size = 16\n",
      "train_dataset : \n",
      "\tlen = 172148\n",
      "\n",
      "\n",
      "valid_dataset : \n",
      "\tlen = 28692\n",
      "\n",
      "\n",
      "\n",
      "** net setting **\n",
      "\tinitial_checkpoint = C:\\Users\\Kaggle\\BengaliAI\\seResNext_gridmask_output\\checkpoint\\00199999_model.pth\n",
      "net=<class 'model.Net'>\n",
      "\n",
      "optimizer\n",
      "  SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.010000000000000002\n",
      "    momentum: 0.0\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "schduler\n",
      "  CyclicScheduler\n",
      "min_lr=0.001, max_lr=0.010, period=10.0, ratio=1.00\n",
      "\n",
      "** start training here! **\n",
      "   batch_size=16,  iter_accum=1\n",
      "                    |----------------------- VALID------------------------------------|------- TRAIN/BATCH -----------\n",
      "rate    iter  epoch | kaggle                    | loss               acc              | loss             | time       \n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "0.00000 200.0  38.2 | 0.981 : 0.974 0.990 0.985 | 0.09, 0.03, 0.03 : 0.98, 0.99, 0.99 | 0.00, 0.00, 0.00 | 0 hr 02 min\n",
      "0.00122 200.2  38.2 | 0.981 : 0.974 0.990 0.985 | 0.09, 0.03, 0.03 : 0.98, 0.99, 0.99 | 0.12, 0.05, 0.04 | 0 hr 04 min\n",
      "0.00122 200.5  38.2 | 0.976 : 0.970 0.984 0.979 | 0.11, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.14, 0.05, 0.08 | 0 hr 08 min\n",
      "0.00122 200.8  38.2 | 0.976 : 0.970 0.984 0.979 | 0.11, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.10, 0.06, 0.05 | 0 hr 09 min\n",
      "0.00122 201.0  38.3 | 0.972 : 0.963 0.987 0.977 | 0.14, 0.04, 0.04 : 0.96, 0.99, 0.99 | 0.20, 0.04, 0.05 | 0 hr 13 min\n",
      "0.00122 201.2  38.3 | 0.972 : 0.963 0.987 0.977 | 0.14, 0.04, 0.04 : 0.96, 0.99, 0.99 | 0.11, 0.04, 0.05 | 0 hr 14 min\n",
      "0.00122 201.5  38.3 | 0.972 : 0.966 0.982 0.975 | 0.12, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.15, 0.07, 0.06 | 0 hr 18 min\n",
      "0.00122 201.8  38.3 | 0.972 : 0.966 0.982 0.975 | 0.12, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.12, 0.04, 0.07 | 0 hr 20 min\n",
      "0.00122 202.0  38.4 | 0.979 : 0.971 0.989 0.985 | 0.10, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.12, 0.06, 0.06 | 0 hr 24 min\n",
      "0.00122 202.2  38.4 | 0.979 : 0.971 0.989 0.985 | 0.10, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.08, 0.06, 0.05 | 0 hr 25 min\n",
      "0.00122 202.5  38.4 | 0.973 : 0.965 0.984 0.980 | 0.13, 0.05, 0.05 : 0.97, 0.99, 0.99 | 0.17, 0.06, 0.04 | 0 hr 29 min\n",
      "0.00122 202.8  38.4 | 0.973 : 0.965 0.984 0.980 | 0.13, 0.05, 0.05 : 0.97, 0.99, 0.99 | 0.10, 0.06, 0.07 | 0 hr 31 min\n",
      "0.00122 203.0  38.5 | 0.973 : 0.963 0.987 0.979 | 0.12, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.12, 0.09, 0.06 | 0 hr 35 min\n",
      "0.00122 203.2  38.5 | 0.973 : 0.963 0.987 0.979 | 0.12, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.13, 0.07, 0.04 | 0 hr 36 min\n",
      "0.00122 203.5  38.5 | 0.976 : 0.968 0.987 0.981 | 0.11, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.14, 0.07, 0.07 | 0 hr 40 min\n",
      "0.00122 203.8  38.5 | 0.976 : 0.968 0.987 0.981 | 0.11, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.11, 0.10, 0.06 | 0 hr 41 min\n",
      "0.00122 204.0  38.5 | 0.973 : 0.964 0.986 0.978 | 0.12, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.16, 0.05, 0.07 | 0 hr 45 min\n",
      "0.00122 204.2  38.6 | 0.973 : 0.964 0.986 0.978 | 0.12, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.11, 0.07, 0.07 | 0 hr 47 min\n",
      "0.00122 204.5  38.6 | 0.974 : 0.967 0.986 0.975 | 0.12, 0.05, 0.05 : 0.97, 0.99, 0.99 | 0.16, 0.05, 0.07 | 0 hr 51 min\n",
      "0.00122 204.8  38.6 | 0.974 : 0.967 0.986 0.975 | 0.12, 0.05, 0.05 : 0.97, 0.99, 0.99 | 0.12, 0.04, 0.06 | 0 hr 52 min\n",
      "0.00122 205.0  38.6 | 0.970 : 0.960 0.983 0.976 | 0.13, 0.05, 0.06 : 0.96, 0.99, 0.98 | 0.11, 0.08, 0.06 | 0 hr 56 min\n",
      "0.00122 205.2  38.7 | 0.970 : 0.960 0.983 0.976 | 0.13, 0.05, 0.06 : 0.96, 0.99, 0.98 | 0.14, 0.08, 0.04 | 0 hr 57 min\n",
      "0.00122 205.5  38.7 | 0.968 : 0.957 0.983 0.975 | 0.15, 0.05, 0.05 : 0.96, 0.99, 0.99 | 0.12, 0.05, 0.07 | 1 hr 02 min\n",
      "0.00122 205.8  38.7 | 0.968 : 0.957 0.983 0.975 | 0.15, 0.05, 0.05 : 0.96, 0.99, 0.99 | 0.13, 0.05, 0.05 | 1 hr 03 min\n",
      "0.00122 206.0  38.7 | 0.973 : 0.967 0.981 0.977 | 0.11, 0.05, 0.05 : 0.97, 0.99, 0.99 | 0.14, 0.05, 0.07 | 1 hr 07 min\n",
      "0.00122 206.2  38.8 | 0.973 : 0.967 0.981 0.977 | 0.11, 0.05, 0.05 : 0.97, 0.99, 0.99 | 0.13, 0.07, 0.06 | 1 hr 08 min\n",
      "0.00122 206.5  38.8 | 0.975 : 0.968 0.983 0.981 | 0.12, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.12, 0.09, 0.07 | 1 hr 12 min\n",
      "0.00122 206.8  38.8 | 0.975 : 0.968 0.983 0.981 | 0.12, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.13, 0.06, 0.05 | 1 hr 14 min\n",
      "0.00122 207.0  38.8 | 0.977 : 0.971 0.987 0.978 | 0.12, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.13, 0.05, 0.04 | 1 hr 18 min\n",
      "0.00122 207.2  38.9 | 0.977 : 0.971 0.987 0.978 | 0.12, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.20, 0.12, 0.05 | 1 hr 19 min\n",
      "0.00122 207.5  38.9 | 0.974 : 0.965 0.986 0.979 | 0.12, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.14, 0.05, 0.08 | 1 hr 23 min\n",
      "0.00122 207.8  38.9 | 0.974 : 0.965 0.986 0.979 | 0.12, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.17, 0.05, 0.07 | 1 hr 24 min\n",
      "0.00122 208.0  38.9 | 0.976 : 0.966 0.987 0.984 | 0.12, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.13, 0.06, 0.05 | 1 hr 28 min\n",
      "0.00122 208.2  38.9 | 0.976 : 0.966 0.987 0.984 | 0.12, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.16, 0.03, 0.04 | 1 hr 30 min\n",
      "0.00122 208.5  39.0 | 0.977 : 0.969 0.987 0.982 | 0.11, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.10, 0.08, 0.04 | 1 hr 34 min\n",
      "0.00122 208.8  39.0 | 0.977 : 0.969 0.987 0.982 | 0.11, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.13, 0.05, 0.06 | 1 hr 35 min\n",
      "0.00122 209.0  39.0 | 0.976 : 0.969 0.988 0.978 | 0.11, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.12, 0.06, 0.07 | 1 hr 39 min\n",
      "0.00122 209.2  39.0 | 0.976 : 0.969 0.988 0.978 | 0.11, 0.05, 0.04 : 0.97, 0.99, 0.99 | 0.16, 0.05, 0.07 | 1 hr 41 min\n",
      "0.00122 209.5  39.1 | 0.976 : 0.970 0.986 0.978 | 0.11, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.16, 0.09, 0.06 | 1 hr 45 min\n",
      "0.00122 209.8  39.1 | 0.976 : 0.970 0.986 0.978 | 0.11, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.12, 0.07, 0.04 | 1 hr 46 min\n",
      "0.00122 210.0  39.1 | 0.975 : 0.968 0.985 0.977 | 0.11, 0.05, 0.05 : 0.97, 0.99, 0.99 | 0.13, 0.05, 0.08 | 1 hr 50 min\n",
      "0.00122 210.2  39.1 | 0.975 : 0.968 0.985 0.977 | 0.11, 0.05, 0.05 : 0.97, 0.99, 0.99 | 0.10, 0.04, 0.08 | 1 hr 51 min\n",
      "0.00122 210.5  39.2 | 0.976 : 0.971 0.985 0.979 | 0.11, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.14, 0.06, 0.08 | 1 hr 55 min\n",
      "0.00122 210.8  39.2 | 0.976 : 0.971 0.985 0.979 | 0.11, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.18, 0.10, 0.06 | 1 hr 57 min\n",
      "0.00186 210.8  39.2 | 0.976 : 0.971 0.985 0.979 | 0.11, 0.04, 0.04 : 0.97, 0.99, 0.99 | 0.07, 0.16, 0.00 | 1 hr 57 min\n"
     ]
    }
   ],
   "source": [
    "run_train(testing=False,augment=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
