{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geffnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "# --- models ---\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import catboost as cb\n",
    "\n",
    "# --- setup ---\n",
    "pd.set_option('max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "submission=False\n",
    "batch_size=64\n",
    "device='cuda'\n",
    "out=r'C:\\Users\\Kaggle\\BengaliAI\\V15_output\\epoch_0_to_150'\n",
    "image_size=64*3\n",
    "arch='pretrained'\n",
    "model_name='efficientnet_b2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "datadir = Path(r'C:\\Users\\Kaggle\\BengaliAI\\inputs\\bengaliai-cv19')\n",
    "featherdir = Path(r'C:\\Users\\Kaggle\\BengaliAI\\inputs\\bengaliaicv19feather')\n",
    "outdir = Path(r'C:\\Users\\Kaggle\\BengaliAI\\V15_output\\epoch_0_to_150')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "def prepare_image(datadir, featherdir, data_type='train',\n",
    "                  submission=False, indices=[0, 1, 2, 3]):\n",
    "    assert data_type in ['train', 'test']\n",
    "    if submission:\n",
    "        image_df_list = [pd.read_parquet(datadir / f'{data_type}_image_data_{i}.parquet')\n",
    "                         for i in indices]\n",
    "    else:\n",
    "        image_df_list = [pd.read_feather(featherdir / f'{data_type}_image_data_{i}.feather')\n",
    "                         for i in indices]\n",
    "\n",
    "    print('image_df_list', len(image_df_list))\n",
    "    HEIGHT = 137\n",
    "    WIDTH = 236\n",
    "    images = [df.iloc[:, 1:].values.reshape(-1, HEIGHT, WIDTH) for df in image_df_list]\n",
    "    del image_df_list\n",
    "    gc.collect()\n",
    "    images = np.concatenate(images, axis=0)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train = pd.read_csv(datadir/'train.csv')\n",
    "train_labels = train[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].values\n",
    "indices = [0] if debug else [0, 1, 2, 3]\n",
    "train_images = prepare_image(\n",
    "    datadir, featherdir, data_type='train', submission=False, indices=indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>\n",
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Referenced `chainer.dataset.DatasetMixin` to work with pytorch Dataset.\n",
    "\"\"\"\n",
    "import numpy\n",
    "import six\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class DatasetMixin(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns an example or a sequence of examples.\"\"\"\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        if isinstance(index, slice):\n",
    "            current, stop, step = index.indices(len(self))\n",
    "            return [self.get_example_wrapper(i) for i in\n",
    "                    six.moves.range(current, stop, step)]\n",
    "        elif isinstance(index, list) or isinstance(index, numpy.ndarray):\n",
    "            return [self.get_example_wrapper(i) for i in index]\n",
    "        else:\n",
    "            return self.get_example_wrapper(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of data points.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_example_wrapper(self, i):\n",
    "        \"\"\"Wrapper of `get_example`, to apply `transform` if necessary\"\"\"\n",
    "        example = self.get_example(i)\n",
    "        if self.transform:\n",
    "            example = self.transform(example)\n",
    "        return example\n",
    "\n",
    "    def get_example(self, i):\n",
    "        \"\"\"Returns the i-th example.\n",
    "\n",
    "        Implementations should override it. It should raise :class:`IndexError`\n",
    "        if the index is invalid.\n",
    "\n",
    "        Args:\n",
    "            i (int): The index of the example.\n",
    "\n",
    "        Returns:\n",
    "            The i-th example.\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class BengaliAIDataset(DatasetMixin):\n",
    "    def __init__(self, images, labels=None, transform=None, indices=None):\n",
    "        super(BengaliAIDataset, self).__init__(transform=transform)\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        if indices is None:\n",
    "            indices = np.arange(len(images))\n",
    "        self.indices = indices\n",
    "        self.train = labels is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"return length of this dataset\"\"\"\n",
    "        return len(self.indices)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        \"\"\"Return i-th data\"\"\"\n",
    "        i = self.indices[i]\n",
    "        x = self.images[i]\n",
    "        # Opposite white and black: background will be white and\n",
    "        # for future Affine transformation\n",
    "        x = (255 - x).astype(np.float32) / 255.\n",
    "        x = x.astype(np.double)\n",
    "        if self.train:\n",
    "            y = self.labels[i]\n",
    "            return x, y\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BengaliAIDataset(train_images, train_labels)\n",
    "\n",
    "image, label = train_dataset[0]\n",
    "print('image', image.shape, 'label', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrow, ncol = 5, 6\n",
    "\n",
    "fig, axes = plt.subplots(nrow, ncol, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "for i, ax in tqdm(enumerate(axes)):\n",
    "    image, label = train_dataset[i]\n",
    "    ax.imshow(image, cmap='Greys')\n",
    "    ax.set_title(f'label: {label}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('bengaliai.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"processing\"></a>\n",
    "# Data augmentation/processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CNN training, data augmentation is important to improve test accuracy (generalization performance). I will show some image preprocessing to increase the data variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From https://www.kaggle.com/corochann/deep-learning-cnn-with-chainer-lb-0-99700\n",
    "\"\"\"\n",
    "import cv2\n",
    "from skimage.transform import AffineTransform, warp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def affine_image(img):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        img: (h, w) or (1, h, w)\n",
    "\n",
    "    Returns:\n",
    "        img: (h, w)\n",
    "    \"\"\"\n",
    "    \n",
    "    #print(\"img.shape:\", img.shape)\n",
    "    \n",
    "    # horizontal flip\n",
    "    #if np.random.rand()<0.5:\n",
    "    #img = img[:,::-1]\n",
    "    \n",
    "    # ch, h, w = img.shape\n",
    "    # img = img / 255.\n",
    "    if img.ndim == 3:\n",
    "        img = img[0]\n",
    "\n",
    "    # --- scale ---\n",
    "    min_scale = 0.8\n",
    "    max_scale = 1.2\n",
    "    sx = np.random.uniform(min_scale, max_scale)\n",
    "    sy = np.random.uniform(min_scale, max_scale)\n",
    "\n",
    "    # --- rotation ---\n",
    "    max_rot_angle = 4\n",
    "    rot_angle = np.random.uniform(-max_rot_angle, max_rot_angle) * np.pi / 180.\n",
    "\n",
    "    # --- shear ---\n",
    "    max_shear_angle = 10\n",
    "    shear_angle = np.random.uniform(-max_shear_angle, max_shear_angle) * np.pi / 180.\n",
    "\n",
    "    # --- translation ---\n",
    "    max_translation = 4\n",
    "    tx = np.random.randint(-max_translation, max_translation)\n",
    "    ty = np.random.randint(-max_translation, max_translation)\n",
    "\n",
    "    #tform = AffineTransform(scale=(sx, sy), rotation=rot_angle, shear=shear_angle,\n",
    "    #                        translation=(tx, ty))\n",
    "    tform = AffineTransform(scale=(sx, sy), shear=shear_angle, rotation=rot_angle)\n",
    "    \n",
    "    transformed_image = warp(img, tform)\n",
    "    assert transformed_image.ndim == 2\n",
    "    return transformed_image\n",
    "\n",
    "\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "def connected_comp(array):\n",
    "    \n",
    "    structure = np.ones((3, 3), dtype=np.int)  # this defines the connection filter\n",
    "    labeled, ncomponents = label(array, structure)\n",
    "    \n",
    "    valid = []\n",
    "    for i in range(1,ncomponents+1):\n",
    "        if np.sum(labeled == i)>10:\n",
    "            valid.append(i)\n",
    "\n",
    "    new_array = np.zeros(array.shape)\n",
    "    for i in valid:\n",
    "        new_array += labeled==i\n",
    "    return new_array\n",
    "\n",
    "def pad_with(vector, pad_width, iaxis, kwargs):\n",
    "    pad_value = kwargs.get('padder', 0)\n",
    "    vector[:pad_width[0]] = pad_value\n",
    "    vector[-pad_width[1]:] = pad_value\n",
    "\n",
    "def crop_char_image(image, threshold=5./255.):\n",
    "    assert image.ndim == 2\n",
    "    \n",
    "    image[image < 20./255.] = 0\n",
    "    \n",
    "    is_black = image > threshold\n",
    "    is_black[:5,:] = 0\n",
    "    is_black[-5:,:] = 0\n",
    "    is_black[:,:5] = 0\n",
    "    is_black[:,-5:] = 0\n",
    "    #is_black = connected_comp(is_black)\n",
    "    is_black_vertical = np.sum(is_black, axis=0) > 0\n",
    "    is_black_horizontal = np.sum(is_black, axis=1) > 0\n",
    "    left = np.argmax(is_black_horizontal)\n",
    "    right = np.argmax(is_black_horizontal[::-1])\n",
    "    top = np.argmax(is_black_vertical)\n",
    "    bottom = np.argmax(is_black_vertical[::-1])\n",
    "    height, width = image.shape\n",
    "    cropped_image = image[left-5:height - right+5, top-5:width - bottom+5]\n",
    "    \n",
    "    lx, ly = height - right - left + 10, width - bottom - top + 10\n",
    "    l = max(lx,ly)\n",
    "    #make sure that the aspect ratio is kept in rescaling\n",
    "    cropped_image = np.pad(cropped_image, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n",
    "    \n",
    "    # scaling\n",
    "    cropped_image = cropped_image*(1/cropped_image.max())\n",
    "    \n",
    "    #pad\n",
    "    cropped_image = np.pad(cropped_image, 8, pad_with)\n",
    "    \n",
    "    return cropped_image\n",
    "\n",
    "def resize(image, size=(128, 128)):\n",
    "    return cv2.resize(image, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erosion\n",
    "def erosion(img):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, tuple(np.random.randint(1, 3, 2)))\n",
    "    img = cv2.erode(img, kernel, iterations=1)\n",
    "    return img\n",
    "\n",
    "# Dilation\n",
    "def dilation(img):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, tuple(np.random.randint(1, 3, 2)))\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_random_kernel():\n",
    "    structure = np.random.choice([cv2.MORPH_RECT, cv2.MORPH_ELLIPSE, cv2.MORPH_CROSS])\n",
    "    kernel = cv2.getStructuringElement(structure, tuple(np.random.randint(1, 3, 2)))\n",
    "    return kernel\n",
    "\n",
    "# Opening\n",
    "def opening(img):\n",
    "    img = cv2.erode(img, get_random_kernel(), iterations=1)\n",
    "    img = cv2.dilate(img, get_random_kernel(), iterations=1)\n",
    "    return img\n",
    "\n",
    "#Closing\n",
    "def closing(img):\n",
    "    img = cv2.dilate(img, get_random_kernel(), iterations=1)\n",
    "    img = cv2.erode(img, get_random_kernel(), iterations=1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrow, ncol = 1, 6\n",
    "\n",
    "fig, axes = plt.subplots(nrow, ncol, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "for i, ax in tqdm(enumerate(axes)):\n",
    "    image, label2 = train_dataset[0]\n",
    "    ax.imshow(affine_image(image), cmap='Greys')\n",
    "    ax.set_title(f'label: {label2}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nrow, ncol = 5, 6\n",
    "\n",
    "fig, axes = plt.subplots(nrow, ncol, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "for i, ax in tqdm(enumerate(axes)):\n",
    "    image, label2 = train_dataset[i+30*5]\n",
    "    ax.imshow(image, cmap='Greys')\n",
    "    ax.set_title(f'label: {label2}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "nrow, ncol = 5, 6\n",
    "\n",
    "fig, axes = plt.subplots(nrow, ncol, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "for i, ax in tqdm(enumerate(axes)):\n",
    "    image, label2 = train_dataset[i+30*5]\n",
    "    ax.imshow(resize(crop_char_image(image, threshold=40./255.)), cmap='Greys')\n",
    "    ax.set_title(f'label: {label2}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything together with `Transform` class. <br>\n",
    "[Update] I added **albumentations augmentations** introduced in [Bengali: albumentations data augmentation tutorial](https://www.kaggle.com/corochann/bengali-albumentations-data-augmentation-tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_gaussian_noise(x, sigma):\n",
    "    x += np.random.randn(*x.shape) * sigma\n",
    "    x = np.clip(x, 0., 1.)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _evaluate_ratio(ratio):\n",
    "    if ratio <= 0.:\n",
    "        return False\n",
    "    return np.random.uniform() < ratio\n",
    "\n",
    "\n",
    "def apply_aug(aug, image):\n",
    "    return aug(image=image)['image']\n",
    "        \n",
    "        \n",
    "class Transform:\n",
    "    def __init__(self, affine=True, crop=False, size=(64, 64),\n",
    "                 normalize=True, train=True, threshold=40.,\n",
    "                 sigma=-1., blur_ratio=0., noise_ratio=0., cutout_ratio=0.,\n",
    "                 grid_distortion_ratio=0., elastic_distortion_ratio=0., random_brightness_ratio=0.,\n",
    "                 piece_affine_ratio=0., ssr_ratio=0.):\n",
    "        self.affine = affine\n",
    "        self.crop = crop\n",
    "        self.size = size\n",
    "        self.normalize = normalize\n",
    "        self.train = train\n",
    "        self.threshold = threshold / 255.\n",
    "        self.sigma = sigma / 255.\n",
    "\n",
    "        self.blur_ratio = blur_ratio\n",
    "        self.noise_ratio = noise_ratio\n",
    "        self.cutout_ratio = cutout_ratio\n",
    "        self.grid_distortion_ratio = grid_distortion_ratio\n",
    "        self.elastic_distortion_ratio = elastic_distortion_ratio\n",
    "        self.random_brightness_ratio = random_brightness_ratio\n",
    "        self.piece_affine_ratio = piece_affine_ratio\n",
    "        self.ssr_ratio = ssr_ratio\n",
    "\n",
    "    def __call__(self, example):\n",
    "        if self.train:\n",
    "            x, y = example\n",
    "        else:\n",
    "            x = example\n",
    "                    \n",
    "        # remove noise\n",
    "        # x [x<28./255.] = 0\n",
    "        \n",
    "        # horizontal flip\n",
    "        #x = x[:,:,::-1,:]\n",
    "\n",
    "        \n",
    "        # --- Augmentation ---\n",
    "        if self.affine:\n",
    "            x = affine_image(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        # --- Train/Test common preprocessing ---\n",
    "        if self.crop:\n",
    "            x = crop_char_image(x, threshold=self.threshold)\n",
    "        if self.size is not None:\n",
    "            x = resize(x, size=self.size)\n",
    "        if self.sigma > 0.:\n",
    "            x = add_gaussian_noise(x, sigma=self.sigma)\n",
    "\n",
    "        # albumentations...\n",
    "        x = x.astype(np.float32)\n",
    "        assert x.ndim == 2\n",
    "        # 1. blur\n",
    "        if _evaluate_ratio(self.blur_ratio):\n",
    "            r = np.random.uniform()\n",
    "            if r < 0.25:\n",
    "                x = apply_aug(A.Blur(p=1.0), x)\n",
    "            elif r < 0.5:\n",
    "                x = apply_aug(A.MedianBlur(blur_limit=5, p=1.0), x)\n",
    "            elif r < 0.75:\n",
    "                x = apply_aug(A.GaussianBlur(p=1.0), x)\n",
    "            else:\n",
    "                x = apply_aug(A.MotionBlur(p=1.0), x)\n",
    "\n",
    "        if _evaluate_ratio(self.noise_ratio):\n",
    "            r = np.random.uniform()\n",
    "            if r < 0.50:\n",
    "                x = apply_aug(A.GaussNoise(var_limit=5. / 255., p=1.0), x)\n",
    "            else:\n",
    "                x = apply_aug(A.MultiplicativeNoise(p=1.0), x)\n",
    "\n",
    "        if _evaluate_ratio(self.cutout_ratio):\n",
    "            # A.Cutout(num_holes=2,  max_h_size=2, max_w_size=2, p=1.0)  # Deprecated...\n",
    "            x = apply_aug(A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=1.0), x)\n",
    "\n",
    "        if _evaluate_ratio(self.grid_distortion_ratio):\n",
    "            x = apply_aug(A.GridDistortion(p=1.0), x)\n",
    "\n",
    "        if _evaluate_ratio(self.elastic_distortion_ratio):\n",
    "            x = apply_aug(A.ElasticTransform(\n",
    "                sigma=50, alpha=1, alpha_affine=10, p=1.0), x)\n",
    "\n",
    "        if _evaluate_ratio(self.random_brightness_ratio):\n",
    "            # A.RandomBrightness(p=1.0)  # Deprecated...\n",
    "            # A.RandomContrast(p=1.0)    # Deprecated...\n",
    "            x = apply_aug(A.RandomBrightnessContrast(p=1.0), x)\n",
    "\n",
    "        if _evaluate_ratio(self.piece_affine_ratio):\n",
    "            x = apply_aug(A.IAAPiecewiseAffine(p=1.0), x)\n",
    "\n",
    "        if _evaluate_ratio(self.ssr_ratio):\n",
    "            x = apply_aug(A.ShiftScaleRotate(\n",
    "                shift_limit=0.01,#0.0625,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=30,\n",
    "                p=1.0), x)\n",
    "\n",
    "            \n",
    "        #plt.imshow(x[0,0].cpu().numpy())\n",
    "        #plt.show()\n",
    "        \n",
    "        #if self.normalize:\n",
    "        #    x = (x.astype(np.float32) - 0.0692) / 0.2051\n",
    "        # normalize by its max val\n",
    "        # x = x * (255./x.max())\n",
    "        \n",
    "        if x.ndim == 2:\n",
    "            x = x[None, :, :]\n",
    "        x = x.astype(np.float32)\n",
    "        if self.train:\n",
    "            y = y.astype(np.int64)\n",
    "            return x, y\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Transform( crop=False,\n",
    "    size=(236, 137), threshold=40.,\n",
    "    sigma=-1., blur_ratio=0, noise_ratio=0, cutout_ratio=0,\n",
    "    grid_distortion_ratio=0, random_brightness_ratio=0,\n",
    "    piece_affine_ratio=0.1, ssr_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BengaliAIDataset(train_images, train_labels,\n",
    "                                 transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout:\n",
    "    def __init__(self, mask_size, p, cutout_inside, mask_color=1):\n",
    "        self.p = p\n",
    "        self.mask_size = mask_size\n",
    "        self.cutout_inside = cutout_inside\n",
    "        self.mask_color = mask_color\n",
    "\n",
    "        self.mask_size_half = mask_size // 2\n",
    "        self.offset = 1 if mask_size % 2 == 0 else 0\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image_all = copy.copy(image)\n",
    "        image = image[0,0]\n",
    "        #image = np.asarray(image).copy()\n",
    "\n",
    "        if np.random.random() > self.p:\n",
    "            return image\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        if self.cutout_inside:\n",
    "            cxmin, cxmax = self.mask_size_half, w + self.offset - self.mask_size_half\n",
    "            cymin, cymax = self.mask_size_half, h + self.offset - self.mask_size_half\n",
    "        else:\n",
    "            cxmin, cxmax = 0, w + self.offset\n",
    "            cymin, cymax = 0, h + self.offset\n",
    "\n",
    "        cx = np.random.randint(cxmin, cxmax)\n",
    "        cy = np.random.randint(cymin, cymax)\n",
    "        xmin = cx - self.mask_size_half\n",
    "        ymin = cy - self.mask_size_half\n",
    "        xmax = xmin + self.mask_size\n",
    "        ymax = ymin + self.mask_size\n",
    "        xmin = max(0, xmin)\n",
    "        ymin = max(0, ymin)\n",
    "        xmax = min(w, xmax)\n",
    "        ymax = min(h, ymax)\n",
    "        \n",
    "        \n",
    "        image_all[:, :, ymin:ymax, xmin:xmax] = self.mask_color\n",
    "        return image_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout = Cutout(80, 1, True, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's final check the processed images, which will be trained by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrow, ncol = 5, 6\n",
    "\n",
    "fig, axes = plt.subplots(nrow, ncol, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "for i, ax in tqdm(enumerate(axes)):\n",
    "    image, label = train_dataset[i]\n",
    "    ax.imshow(image[0], cmap='Greys')\n",
    "    ax.set_title(f'label: {label}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a> \n",
    "# pytorch model & define classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def residual_add(lhs, rhs):\n",
    "    lhs_ch, rhs_ch = lhs.shape[1], rhs.shape[1]\n",
    "    if lhs_ch < rhs_ch:\n",
    "        out = lhs + rhs[:, :lhs_ch]\n",
    "    elif lhs_ch > rhs_ch:\n",
    "        out = torch.cat([lhs[:, :rhs_ch] + rhs, lhs[:, rhs_ch:]], dim=1)\n",
    "    else:\n",
    "        out = lhs + rhs\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class LazyLoadModule(nn.Module):\n",
    "    \"\"\"Lazy buffer/parameter loading using load_state_dict_pre_hook\n",
    "\n",
    "    Define all buffer/parameter in `_lazy_buffer_keys`/`_lazy_parameter_keys` and\n",
    "    save buffer with `register_buffer`/`register_parameter`\n",
    "    method, which can be outside of __init__ method.\n",
    "    Then this module can load any shape of Tensor during de-serializing.\n",
    "\n",
    "    Note that default value of lazy buffer is torch.Tensor([]), while lazy parameter is None.\n",
    "    \"\"\"\n",
    "    _lazy_buffer_keys: List[str] = []     # It needs to be override to register lazy buffer\n",
    "    _lazy_parameter_keys: List[str] = []  # It needs to be override to register lazy parameter\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LazyLoadModule, self).__init__()\n",
    "        for k in self._lazy_buffer_keys:\n",
    "            self.register_buffer(k, torch.tensor([]))\n",
    "        for k in self._lazy_parameter_keys:\n",
    "            self.register_parameter(k, None)\n",
    "        self._register_load_state_dict_pre_hook(self._hook)\n",
    "\n",
    "    def _hook(self, state_dict, prefix, local_metadata, strict, missing_keys,\n",
    "             unexpected_keys, error_msgs):\n",
    "        for key in self._lazy_buffer_keys:\n",
    "            self.register_buffer(key, state_dict[prefix + key])\n",
    "\n",
    "        for key in self._lazy_parameter_keys:\n",
    "            self.register_parameter(key, Parameter(state_dict[prefix + key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn import init\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LazyLinear(LazyLoadModule):\n",
    "    \"\"\"Linear module with lazy input inference\n",
    "\n",
    "    `in_features` can be `None`, and it is determined at the first time of forward step dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = ['bias', 'in_features', 'out_features']\n",
    "    _lazy_parameter_keys = ['weight']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(LazyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if in_features is not None:\n",
    "            self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "            self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.weight is None:\n",
    "            self.in_features = input.shape[-1]\n",
    "            self.weight = Parameter(torch.Tensor(self.out_features, self.in_features))\n",
    "            self.reset_parameters()\n",
    "\n",
    "            # Need to send lazy defined parameter to device...\n",
    "            self.to(input.device)\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True,\n",
    "                 use_bn=True, activation=F.relu, dropout_ratio=-1, residual=False,):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        if in_features is None:\n",
    "            self.linear = LazyLinear(in_features, out_features, bias=bias)\n",
    "        else:\n",
    "            self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        if use_bn:\n",
    "            self.bn = nn.BatchNorm1d(out_features)\n",
    "        if dropout_ratio > 0.:\n",
    "            self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        self.activation = activation\n",
    "        self.use_bn = use_bn\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.residual = residual\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            h = self.bn(h)\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        if self.residual:\n",
    "            h = residual_add(h, x)\n",
    "        if self.dropout_ratio > 0:\n",
    "            h = self.dropout(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEM\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "class GeM(nn.Module):\n",
    "\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretrainedmodels\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential\n",
    "\n",
    "\n",
    "class PretrainedCNN(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet_b2',\n",
    "                 in_channels=1, out_dim=10, use_bn=True,\n",
    "                 pretrained='imagenet'):\n",
    "        super(PretrainedCNN, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(\n",
    "            in_channels, 3, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "#         self.base_model = pretrainedmodels.__dict__[model_name](pretrained=pretrained)\n",
    "        self.base_model = geffnet.create_model(model_name, pretrained=True)\n",
    "        activation = F.leaky_relu\n",
    "        self.do_pooling = True\n",
    "        if self.do_pooling:\n",
    "#             inch = self.base_model.last_linear.in_features\n",
    "            inch = self.base_model.classifier.in_features\n",
    "        else:\n",
    "            inch = None\n",
    "        hdim = 512\n",
    "        lin1_1 = LinearBlock(inch, hdim, use_bn=use_bn, activation=activation, residual=False)\n",
    "        lin1_2 = LinearBlock(hdim, 168, use_bn=use_bn, activation=None, residual=False)\n",
    "        self.lin_layers1 = Sequential(lin1_1, lin1_2)\n",
    "        \n",
    "        lin2_1 = LinearBlock(inch, hdim, use_bn=use_bn, activation=activation, residual=False)\n",
    "        lin2_2 = LinearBlock(hdim, 11, use_bn=use_bn, activation=None, residual=False)\n",
    "        self.lin_layers2 = Sequential(lin2_1, lin2_2)\n",
    "\n",
    "        lin3_1 = LinearBlock(inch, hdim, use_bn=use_bn, activation=activation, residual=False)\n",
    "        lin3_2 = LinearBlock(hdim, 7, use_bn=use_bn, activation=None, residual=False)\n",
    "        self.lin_layers3 = Sequential(lin3_1, lin3_2)\n",
    "\n",
    "        self.pool = GeM()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        h = self.conv0(x)\n",
    "        h = self.base_model.features(h)\n",
    "        \n",
    "        if self.do_pooling:\n",
    "\n",
    "            h1 = self.pool(h)[:,:,0,0]\n",
    "            h2 = self.pool(h)[:,:,0,0]\n",
    "            h3 = self.pool(h)[:,:,0,0]\n",
    "\n",
    "            \n",
    "        else:\n",
    "            # [128, 2048, 4, 4] when input is (128, 128)\n",
    "            bs, ch, height, width = h.shape\n",
    "            h1 = h.view(bs, ch*height*width)\n",
    "            h2 = h.view(bs, ch*height*width)\n",
    "            h3 = h.view(bs, ch*height*width)\n",
    "        for layer in self.lin_layers1:\n",
    "            h1 = layer(h1)\n",
    "        for layer in self.lin_layers2:\n",
    "            h2 = layer(h2)\n",
    "        for layer in self.lin_layers3:\n",
    "            h3 = layer(h3)\n",
    "            \n",
    "            \n",
    "        return h1, h2, h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohem_loss( rate, cls_pred, cls_target ):\n",
    "\n",
    "    batch_size = cls_pred.size(0) \n",
    "    ohem_cls_loss = F.cross_entropy(cls_pred, cls_target, reduction='none', ignore_index=-1)\n",
    "\n",
    "    sorted_ohem_loss, idx = torch.sort(ohem_cls_loss, descending=True)\n",
    "    keep_num = min(sorted_ohem_loss.size()[0], int(batch_size*rate) )\n",
    "    if keep_num < sorted_ohem_loss.size()[0]:\n",
    "        keep_idx_cuda = idx[:keep_num]\n",
    "        ohem_cls_loss = ohem_cls_loss[keep_idx_cuda]\n",
    "    cls_loss = ohem_cls_loss.sum() / keep_num\n",
    "    return cls_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "def cutmix(data, targets1, targets2, targets3, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets1 = targets1[indices]\n",
    "    shuffled_targets2 = targets2[indices]\n",
    "    shuffled_targets3 = targets3[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    while lam > 0.999:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    \n",
    "#     print(lam)\n",
    "#     print(data.shape)\n",
    "#     print(indices)\n",
    "#     print(bbx1, bbx2, bby1, bby2)\n",
    "    \n",
    "    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "\n",
    "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, shuffled_targets3, lam]\n",
    "    return data, targets\n",
    "\n",
    "def mixup(data, targets1, targets2, targets3, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets1 = targets1[indices]\n",
    "    shuffled_targets2 = targets2[indices]\n",
    "    shuffled_targets3 = targets3[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    data = data * lam + shuffled_data * (1 - lam)\n",
    "    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, shuffled_targets3, lam]\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "\n",
    "def cutmix_criterion(preds1,preds2,preds3, targets):\n",
    "    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2) + lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4) + lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)\n",
    "\n",
    "# def cutmix_criterion(preds1,preds2,preds3, targets, rate=0.5):\n",
    "#     targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n",
    "#     criterion = ohem_loss\n",
    "#     return lam * criterion(rate, preds1, targets1) + (1 - lam) * criterion(rate, preds1, targets2), lam * criterion(rate, preds2, targets3) + (1 - lam) * criterion(rate, preds2, targets4), lam * criterion(rate, preds3, targets5) + (1 - lam) * criterion(rate, preds3, targets6)\n",
    "\n",
    "def mixup_criterion(preds1,preds2,preds3, targets):\n",
    "    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    return lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2) + lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4) + lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)\n",
    "\n",
    "\n",
    "# def mixup_criterion(preds1,preds2,preds3, targets, rate=0.5):\n",
    "#     targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n",
    "#     criterion = ohem_loss\n",
    "#     return lam * criterion(rate, preds1, targets1) + (1 - lam) * criterion(rate, preds1, targets2), lam * criterion(rate, preds2, targets3) + (1 - lam) * criterion(rate, preds2, targets4), lam * criterion(rate, preds3, targets5) + (1 - lam) * criterion(rate, preds3, targets6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def accuracy(y, t):\n",
    "    pred_label = torch.argmax(y, dim=1)\n",
    "    count = pred_label.shape[0]\n",
    "    correct = (pred_label == t).sum().type(torch.double)\n",
    "    acc = correct / count\n",
    "    return acc\n",
    "\n",
    "\n",
    "class BengaliClassifier(nn.Module):\n",
    "    def __init__(self, predictor, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
    "        super(BengaliClassifier, self).__init__()\n",
    "        self.n_grapheme = n_grapheme\n",
    "        self.n_vowel = n_vowel\n",
    "        self.n_consonant = n_consonant\n",
    "        self.n_total_class = self.n_grapheme + self.n_vowel + self.n_consonant\n",
    "        self.predictor = predictor.float()\n",
    "        \n",
    "        self.metrics_keys = [\n",
    "            'loss', 'loss_grapheme', 'loss_vowel', 'loss_consonant',\n",
    "            'acc_grapheme', 'acc_vowel', 'acc_consonant']\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        \n",
    "        const = np.random.randint(6)\n",
    "        if const<2:        \n",
    "            # mixup\n",
    "            data, targets = mixup(x, y[:,0], y[:,1], y[:,2], 0.4)\n",
    "            x = data\n",
    "        elif const<4:        \n",
    "            # cutmix\n",
    "            data, targets = cutmix(x, y[:,0], y[:,1], y[:,2], 0.4)\n",
    "            x = data\n",
    "        else:\n",
    "            # cutout\n",
    "            x = cutout(x)\n",
    "        \n",
    "        \n",
    "        pred = self.predictor(x)\n",
    "        \n",
    "        \n",
    "        if isinstance(pred, tuple):\n",
    "            assert len(pred) == 3\n",
    "            preds = pred\n",
    "        else:\n",
    "            assert pred.shape[1] == self.n_grapheme #self.n_total_class\n",
    "            preds = pred #torch.split(pred, [self.n_grapheme, self.n_vowel, self.n_consonant], dim=1)\n",
    "            \n",
    "        if const<4:\n",
    "            # cutmix or mixup\n",
    "            targets1, targets2, targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]        \n",
    "            preds1, preds2, preds3 = preds[0], preds[1], preds[2]\n",
    "            lam = float(lam)\n",
    "            loss_grapheme = lam * F.cross_entropy(preds1, targets1) + (1 - lam) * F.cross_entropy(preds1, targets2) \n",
    "            loss_vowel = lam * F.cross_entropy(preds2, targets3) + (1 - lam) * F.cross_entropy(preds2, targets4)\n",
    "            loss_consonant = lam * F.cross_entropy(preds3, targets5) + (1 - lam) * F.cross_entropy(preds3, targets6)\n",
    "            loss = 2*loss_grapheme + loss_vowel + loss_consonant\n",
    "            #loss = loss_grapheme + loss_vowel + loss_consonant\n",
    "        else:\n",
    "            loss_grapheme = F.cross_entropy(preds[0], y[:, 0])\n",
    "            loss_vowel = F.cross_entropy(preds[1], y[:, 1])\n",
    "            loss_consonant = F.cross_entropy(preds[2], y[:, 2])\n",
    "            loss = 2*loss_grapheme + loss_vowel + loss_consonant\n",
    "            #loss = loss_grapheme + loss_vowel + loss_consonant\n",
    "        metrics = {\n",
    "            'loss': loss.item(),\n",
    "            'loss_grapheme': loss_grapheme.item(),\n",
    "            'loss_vowel': loss_vowel.item(),\n",
    "            'loss_consonant': loss_consonant.item(),\n",
    "            'acc_grapheme': accuracy(preds[0], y[:, 0]),\n",
    "            'acc_vowel': accuracy(preds[1], y[:, 1]),\n",
    "            'acc_consonant': accuracy(preds[2], y[:, 2]),\n",
    "        }\n",
    "        \n",
    "        return loss, metrics, pred\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward_val(self, x, y=None):\n",
    "        \n",
    "        \n",
    "        pred = self.predictor(x)\n",
    "        #pred1 = self.predictor(x[:,:,::-1,:])\n",
    "        #pred2 = self.predictor(x)\n",
    "        #pred = (pred1+pred2)/2 \n",
    "        \n",
    "        if isinstance(pred, tuple):\n",
    "            assert len(pred) == 3\n",
    "            preds = pred\n",
    "        else:\n",
    "            assert pred.shape[1] == self.n_total_class\n",
    "            preds = torch.split(pred, [self.n_grapheme, self.n_vowel, self.n_consonant], dim=1)\n",
    "        \n",
    "        loss_grapheme = F.cross_entropy(preds[0], y[:, 0])\n",
    "        loss_vowel = F.cross_entropy(preds[1], y[:, 1])\n",
    "        loss_consonant = F.cross_entropy(preds[2], y[:, 2])\n",
    "        loss = 2*loss_grapheme + loss_vowel + loss_consonant\n",
    "        #loss = loss_grapheme + loss_vowel + loss_consonant\n",
    "        metrics = {\n",
    "            'loss': loss.item(),\n",
    "            'loss_grapheme': loss_grapheme.item(),\n",
    "            'loss_vowel': loss_vowel.item(),\n",
    "            'loss_consonant': loss_consonant.item(),\n",
    "            'acc_grapheme': accuracy(preds[0], y[:, 0]),\n",
    "            'acc_vowel': accuracy(preds[1], y[:, 1]),\n",
    "            'acc_consonant': accuracy(preds[2], y[:, 2]),\n",
    "        }\n",
    "        \n",
    "        return loss, metrics, pred\n",
    "    \n",
    "\n",
    "    def calc(self, data_loader):\n",
    "        device: torch.device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "        output_list = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader):\n",
    "                # TODO: support general preprocessing.\n",
    "                # If `data` is not `Data` instance, `to` method is not supported!\n",
    "                batch = batch.to(device)\n",
    "                pred = self.predictor(batch)\n",
    "                \n",
    "                output_list.append(pred)\n",
    "        output = torch.cat(output_list, dim=0)\n",
    "        preds = torch.split(output, [self.n_grapheme, self.n_vowel, self.n_consonant], dim=1)\n",
    "        return preds\n",
    "\n",
    "    def predict_proba(self, data_loader):\n",
    "        preds = self.calc(data_loader)\n",
    "        return [F.softmax(p, dim=1) for p in preds]\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        preds = self.calc(data_loader)\n",
    "        pred_labels = [torch.argmax(p, dim=1) for p in preds]\n",
    "        return pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "# Training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import iterstrat\n",
    "import geffnet\n",
    "#!pip install iterative-stratification\n",
    "\n",
    "#get data\n",
    "nfold = 10\n",
    "seed = 22\n",
    "\n",
    "train_df = copy.copy(train)\n",
    "train_df['id'] = train_df['image_id'].apply(lambda x: int(x.split('_')[1]))\n",
    "\n",
    "X, y = train_df[['id', 'grapheme_root', 'vowel_diacritic', 'consonant_diacritic']]\\\n",
    ".values[:,0], train_df.values[:,1:]\n",
    "\n",
    "train_df['fold'] = np.nan\n",
    "\n",
    "#split data\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "mskf = MultilabelStratifiedKFold(n_splits=nfold, random_state=seed)\n",
    "for i, (_, test_index) in enumerate(mskf.split(X, y)):\n",
    "    train_df.iloc[test_index, -1] = i\n",
    "    \n",
    "train_df['fold'] = train_df['fold'].astype('int')\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "n_dataset = len(train_images)\n",
    "train_data_size = 200 if debug else int(n_dataset * 0.8)\n",
    "valid_data_size = 100 if debug else int(n_dataset - train_data_size)\n",
    "\n",
    "#perm = np.random.RandomState(777).permutation(n_dataset)\n",
    "#print('perm', perm)\n",
    "train_dataset = BengaliAIDataset(\n",
    "    train_images, train_labels, transform=train_transform,\n",
    "    indices = np.array(list(train_df[train_df['fold']!=1].index)))\n",
    "    #    indices=perm[:train_data_size])\n",
    "valid_dataset = BengaliAIDataset(\n",
    "    train_images, train_labels, transform=Transform(affine=False, crop=False, size=(236, 137)),\n",
    "    indices = np.array(list(train_df[train_df['fold']==1].index)))\n",
    "#    indices=perm[train_data_size:train_data_size+valid_data_size])\n",
    "print('train_dataset', len(train_dataset), 'valid_dataset', len(valid_dataset))\n",
    "\n",
    "# --- Model ---\n",
    "device = torch.device(device)\n",
    "n_grapheme = 168\n",
    "n_vowel = 11\n",
    "n_consonant = 7\n",
    "n_total = n_grapheme + n_vowel + n_consonant\n",
    "print('n_total', n_total)\n",
    "# Set pretrained='imagenet' to download imagenet pretrained model...\n",
    "predictor = PretrainedCNN(in_channels=1, out_dim=n_total, model_name='efficientnet_b2', pretrained='imagenet')\n",
    "print('predictor', type(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #load weights\n",
    "# predictor.train()\n",
    "# predictor.load_state_dict(torch.load(r\"C:\\Users\\Kaggle\\BengaliAI\\v14_output\\epochs_0_to_150\\models\\model_000150.pt\"))\n",
    "# predictor.train()\n",
    "\n",
    "classifier = BengaliClassifier(predictor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignite utility\n",
    "\n",
    "pytorch-ignite utility class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from logging import getLogger\n",
    "import numpy\n",
    "\n",
    "def save_json(filepath, params):\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(params, f, indent=4)\n",
    "\n",
    "import os\n",
    "from logging import getLogger\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "# from chainer_chemistry.utils import save_json\n",
    "\n",
    "from ignite.engine.engine import Engine, Events\n",
    "from ignite.metrics import Average\n",
    "\n",
    "\n",
    "class DictOutputTransform:\n",
    "    def __init__(self, key, index=0):\n",
    "        self.key = key\n",
    "        self.index = index\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.index >= 0:\n",
    "            x = x[self.index]\n",
    "        return x[self.key]\n",
    "\n",
    "\n",
    "def create_trainer(classifier, optimizer, device):\n",
    "    classifier.to(device)    \n",
    "    def update_fn(engine, batch):\n",
    "        classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # batch = [elem.to(device) for elem in batch]\n",
    "        x, y = [elem.to(device) for elem in batch]\n",
    "        x = x.float()\n",
    "        \n",
    "\n",
    "        loss, metrics, pred_y = classifier(x, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return metrics, pred_y, y\n",
    "    \n",
    "    \n",
    "    trainer = Engine(update_fn)\n",
    "    \n",
    "    for key in classifier.metrics_keys:\n",
    "        Average(output_transform=DictOutputTransform(key)).attach(trainer, key)\n",
    "        \n",
    "    return trainer\n",
    "\n",
    "\n",
    "def create_evaluator(classifier, device):\n",
    "    classifier.to(device)\n",
    "    def update_fn(engine, batch):\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            # batch = [elem.to(device) for elem in batch]\n",
    "            x, y = [elem.to(device) for elem in batch]\n",
    "            x = x.float()\n",
    "            _, metrics, pred_y = classifier.forward_val(x, y)\n",
    "            return metrics, pred_y, y\n",
    "    evaluator = Engine(update_fn)\n",
    "    for key in classifier.metrics_keys:\n",
    "        Average(output_transform=DictOutputTransform(key)).attach(evaluator, key)\n",
    "    return evaluator\n",
    "\n",
    "\n",
    "class LogReport:\n",
    "    def __init__(self, evaluator=None, dirpath=None, logger=None):\n",
    "        self.evaluator = evaluator\n",
    "        self.dirpath = str(dirpath) if dirpath is not None else None\n",
    "        self.logger = logger or getLogger(__name__)\n",
    "\n",
    "        self.reported_dict = {}  # To handle additional parameter to monitor\n",
    "        self.history = []\n",
    "        self.start_time = perf_counter()\n",
    "\n",
    "    def report(self, key, value):\n",
    "        self.reported_dict[key] = value\n",
    "\n",
    "    def __call__(self, engine):\n",
    "        elapsed_time = perf_counter() - self.start_time\n",
    "        elem = {'epoch': engine.state.epoch,\n",
    "                'iteration': engine.state.iteration}\n",
    "        elem.update({f'train/{key}': value\n",
    "                     for key, value in engine.state.metrics.items()})\n",
    "        if self.evaluator is not None:\n",
    "            elem.update({f'valid/{key}': value\n",
    "                         for key, value in self.evaluator.state.metrics.items()})\n",
    "        elem.update(self.reported_dict)\n",
    "        elem['elapsed_time'] = elapsed_time\n",
    "        \n",
    "        ####\n",
    "        print(elem)\n",
    "        \n",
    "        self.history.append(elem)\n",
    "        if self.dirpath:\n",
    "            save_json(os.path.join(self.dirpath, 'log.json'), self.history)\n",
    "            self.get_dataframe().to_csv(os.path.join(self.dirpath, 'log.csv'), index=False)\n",
    "\n",
    "        # --- print ---\n",
    "        msg = ''\n",
    "        for key, value in elem.items():\n",
    "            if key in ['iteration']:\n",
    "                # skip printing some parameters...\n",
    "                continue\n",
    "            elif isinstance(value, int):\n",
    "                msg += f'{key} {value: >6d} '\n",
    "            else:\n",
    "                msg += f'{key} {value: 8f} '\n",
    "#         self.logger.warning(msg)\n",
    "        print(msg)\n",
    "\n",
    "        # --- Reset ---\n",
    "        self.reported_dict = {}\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        df = pd.DataFrame(self.history)\n",
    "        return df\n",
    "\n",
    "\n",
    "class SpeedCheckHandler:\n",
    "    def __init__(self, iteration_interval=10, logger=None):\n",
    "        self.iteration_interval = iteration_interval\n",
    "        self.logger = logger or getLogger(__name__)\n",
    "        self.prev_time = perf_counter()\n",
    "\n",
    "    def __call__(self, engine: Engine):\n",
    "        if engine.state.iteration % self.iteration_interval == 0:\n",
    "            cur_time = perf_counter()\n",
    "            spd = self.iteration_interval / (cur_time - self.prev_time)\n",
    "            self.logger.warning(f'{spd} iter/sec')\n",
    "            # reset\n",
    "            self.prev_time = cur_time\n",
    "\n",
    "    def attach(self, engine: Engine):\n",
    "        engine.add_event_handler(Events.ITERATION_COMPLETED, self)\n",
    "\n",
    "\n",
    "class ModelSnapshotHandler:\n",
    "    def __init__(self, model, filepath=r'C:\\Users\\Kaggle\\BengaliAI\\V15_output\\epoch_0_to_150\\models\\model_{count:06}.pt',\n",
    "                 interval=1, logger=None):\n",
    "        self.model = model\n",
    "        self.filepath: str = str(filepath)\n",
    "        self.interval = interval\n",
    "        self.logger = logger or getLogger(__name__)\n",
    "        self.count = 0\n",
    "\n",
    "    def __call__(self, engine: Engine):\n",
    "        self.count += 1\n",
    "        if self.count % self.interval == 0:\n",
    "            filepath = self.filepath.format(count=self.count)\n",
    "            torch.save(self.model.state_dict(), filepath)\n",
    "            print(\"saving model epoch:\", self.count)\n",
    "            \n",
    "            #self.logger.warning(f'save model to {filepath}...')\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "from ignite.metrics.metric import Metric\n",
    "\n",
    "\n",
    "class EpochMetric(Metric):\n",
    "    \"\"\"Class for metrics that should be computed on the entire output history of a model.\n",
    "    Model's output and targets are restricted to be of shape `(batch_size, n_classes)`. Output\n",
    "    datatype should be `float32`. Target datatype should be `long`.\n",
    "\n",
    "    .. warning::\n",
    "\n",
    "        Current implementation stores all input data (output and target) in as tensors before computing a metric.\n",
    "        This can potentially lead to a memory error if the input data is larger than available RAM.\n",
    "\n",
    "\n",
    "    - `update` must receive output of the form `(y_pred, y)`.\n",
    "\n",
    "    If target shape is `(batch_size, n_classes)` and `n_classes > 1` than it should be binary: e.g. `[[0, 1, 0, 1], ]`.\n",
    "\n",
    "    Args:\n",
    "        compute_fn (callable): a callable with the signature (`torch.tensor`, `torch.tensor`) takes as the input\n",
    "            `predictions` and `targets` and returns a scalar.\n",
    "        output_transform (callable, optional): a callable that is used to transform the\n",
    "            :class:`~ignite.engine.Engine`'s `process_function`'s output into the\n",
    "            form expected by the metric. This can be useful if, for example, you have a multi-output model and\n",
    "            you want to compute the metric with respect to one of the outputs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, compute_fn, output_transform=lambda x: x):\n",
    "\n",
    "        if not callable(compute_fn):\n",
    "            raise TypeError(\"Argument compute_fn should be callable.\")\n",
    "\n",
    "        super(EpochMetric, self).__init__(output_transform=output_transform)\n",
    "        self.compute_fn = compute_fn\n",
    "\n",
    "    def reset(self):\n",
    "        self._predictions = torch.tensor([], dtype=torch.float)\n",
    "        self._targets = torch.tensor([], dtype=torch.long)\n",
    "\n",
    "    def update(self, output):\n",
    "        y_pred, y = output\n",
    "        self._predictions = torch.cat([self._predictions, y_pred], dim=0)\n",
    "        self._targets = torch.cat([self._targets, y], dim=0)\n",
    "\n",
    "        # Check once the signature and execution of compute_fn\n",
    "        if self._predictions.shape == y_pred.shape:\n",
    "            try:\n",
    "                self.compute_fn(self._predictions, self._targets)\n",
    "            except Exception as e:\n",
    "                warnings.warn(\"Probably, there can be a problem with `compute_fn`:\\n {}.\".format(e),\n",
    "                              RuntimeWarning)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.compute_fn(self._predictions, self._targets)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "\n",
    "\n",
    "def macro_recall(pred_y, y, n_grapheme=168, n_vowel=11, n_consonant=7):\n",
    "    pred_y = torch.split(pred_y, [n_grapheme, n_vowel, n_consonant], dim=1)\n",
    "    pred_labels = [torch.argmax(py, dim=1).cpu().numpy() for py in pred_y]\n",
    "\n",
    "    y = y.cpu().numpy()\n",
    "    # pred_y = [p.cpu().numpy() for p in pred_y]\n",
    "\n",
    "    recall_grapheme = sklearn.metrics.recall_score(pred_labels[0], y[:, 0], average='macro')\n",
    "    recall_vowel = sklearn.metrics.recall_score(pred_labels[1], y[:, 1], average='macro')\n",
    "    recall_consonant = sklearn.metrics.recall_score(pred_labels[2], y[:, 2], average='macro')\n",
    "    scores = [recall_grapheme, recall_vowel, recall_consonant]\n",
    "    final_score = np.average(scores, weights=[2, 1, 1])\n",
    "    # print(f'recall: grapheme {recall_grapheme}, vowel {recall_vowel}, consonant {recall_consonant}, '\n",
    "    #       f'total {final_score}, y {y.shape}')\n",
    "    return final_score\n",
    "\n",
    "\n",
    "def calc_macro_recall(solution, submission):\n",
    "    # solution df, submission df\n",
    "    scores = []\n",
    "    for component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n",
    "        y_true_subset = solution[solution[component] == component]['target'].values\n",
    "        y_pred_subset = submission[submission[component] == component]['target'].values\n",
    "        scores.append(sklearn.metrics.recall_score(\n",
    "            y_true_subset, y_pred_subset, average='macro'))\n",
    "    final_score = np.average(scores, weights=[2, 1, 1])\n",
    "    return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from distutils.util import strtobool\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Events\n",
    "from numpy.random.mtrand import RandomState\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# --- Training setting ---\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# AdamW\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=0.005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.7, patience=5, min_lr=1e-10)\n",
    "\n",
    "trainer = create_trainer(classifier, optimizer, device)\n",
    "def output_transform(output):\n",
    "    metric, pred_y, y = output\n",
    "    pred_y = torch.cat((pred_y[0], pred_y[1], pred_y[2]), 1)\n",
    "    \n",
    "    return pred_y.cpu(), y.cpu()\n",
    "EpochMetric(\n",
    "    compute_fn=macro_recall,\n",
    "    output_transform=output_transform\n",
    ").attach(trainer, 'recall')\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.attach(trainer, metric_names='all')\n",
    "\n",
    "evaluator = create_evaluator(classifier, device)\n",
    "EpochMetric(\n",
    "    compute_fn=macro_recall,\n",
    "    output_transform=output_transform\n",
    ").attach(evaluator, 'recall')\n",
    "\n",
    "def run_evaluator(engine):\n",
    "    evaluator.run(valid_loader)\n",
    "\n",
    "def schedule_lr(engine):\n",
    "    # metrics = evaluator.state.metrics\n",
    "    metrics = engine.state.metrics\n",
    "    avg_mae = metrics['loss']\n",
    "\n",
    "    # --- update lr ---\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(avg_mae)\n",
    "    log_report.report('lr', lr)\n",
    "\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, run_evaluator)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, schedule_lr)\n",
    "log_report = LogReport(evaluator, outdir)\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, log_report)\n",
    "trainer.add_event_handler(\n",
    "    Events.EPOCH_COMPLETED,\n",
    "    ModelSnapshotHandler(predictor))\n",
    "    #ModelSnapshotHandler(predictor, filepath=outdir / 'predictor.pt'))\n",
    "#trainer.add_event_handler(Events.EPOCH_COMPLETED, engine_checkpoint, {'mymodel':classifier})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.run(train_loader, max_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = log_report.get_dataframe()\n",
    "train_history.to_csv(outdir / 'log.csv', index=False)\n",
    "\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
